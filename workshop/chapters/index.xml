<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lecture chapters on</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/</link><description>Recent content in Lecture chapters on</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 25 Jan 2023 14:41:39 +0100</lastBuildDate><atom:link href="https://openfoam-parallelisation-course.github.io/workshop/chapters/index.xml" rel="self" type="application/rss+xml"/><item><title>Point-to-Point communications - general introdution</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/01-p2p-comms/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/01-p2p-comms/</guid><description>Lecture video # üëã Content will be available on the the next training day Register at eveeno.com Module transcript Welcome again to this section where we discuss some concepts concerning point-to-point communication in MPI protocols.
The first thing we need to take care of is: how can we identify the different processes which participate in any type of communication?
To this end, MPI defines two types of objects; the first one is called &amp;ldquo;a communicator&amp;rdquo;, which basically groups processes, and identifies the communications between its members.</description></item><item><title>Blocking P2P comms</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/p2p-comms-blocking/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/p2p-comms-blocking/</guid><description>Lecture video # None Left Right Sidebar 0 of 0 Try to play me :) Loading content.
Module transcript Continuing from the last module, we&amp;rsquo;ve asked two questions:
First, should there be a check for correct placement of the tyre? The answer for sure is absolutely; in all cases, at some point before attempting to move the vehicle, someone or something needs to check if the tyre was correctly mounted.</description></item><item><title>Non-Blocking P2P comms</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/p2p-comms-nonblocking/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/p2p-comms-nonblocking/</guid><description>Lecture video # None Left Right Sidebar 0 of 0 Try to play me :) Loading content.
Module transcript In the previous module, we&amp;rsquo;ve talked about an inherent issue with blocking comms: the possibility of a deadlock.
To find a radical solution to this problem, and not requiring a specific order of send and receive operations, the MPI calls need to return without actually sending the data. For this type of comms, OpenFOAM uses Pstream::nonBlocking.</description></item><item><title>P2P comms - Quiz</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/01-p2p-comms-quiz/</link><pubDate>Wed, 25 Jan 2023 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/01-p2p-comms-quiz/</guid><description>--- primary_color: cyan secondary_color: lightgray text_color: black shuffle_questions: false shuffle_answers: true --- ## Basic P2P comms Consider the following code snippet, featuring a short communication between two processes: ```java if (Pstream::master()) { IPstream fromSlave (Pstream::commsTypes::blocking, 1); label i = readLabel(fromSlave); } else if (Pstream::myProcNo() == 1) { OPstream toMaster (Pstream::commsTypes::blocking, Pstream::masterNo()); toMaster &lt;&lt; 5; } ``` The value of `i` on **master** process will be: - [ ] 0 - [x] 5 - [ ] The master process never defines `i` ## Careful, not to run into issues!</description></item><item><title>Collective communications - general introdution</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/02-collective-comms/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/02-collective-comms/</guid><description>Lecture video # üëã Content will be available on the the next training day Register at eveeno.com Module transcript Welcome again to this module where we introduce an alternative communication protocol for when many processes need to talk to each other at the same time: Collective communication.
So, it&amp;rsquo;s not about when two processes communicate with a send/receive pair, but this is a completely different protocol, which has some requirements I want to bring to your attention:</description></item><item><title>Collective comms API</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/collective-comms-api/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/collective-comms-api/</guid><description>Lecture video # None Left Right Sidebar 0 of 0 Try to play me :) Loading content.
Module transcript Now that we introduced collective communication, we&amp;rsquo;ll want to learn how to use them when needed, so in this module, we will take a look at the code interface to these methods.
I want to mention that I will try and follow OpenFOAM&amp;rsquo;s terminology when naming things, which is a little different than MPI wording, so if you&amp;rsquo;re familiar with how things are called in MPI context, it might seem a bit strange at first.</description></item><item><title>Collective comms - Quiz</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/02-collective-comms-quiz/</link><pubDate>Wed, 25 Jan 2023 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/02-collective-comms-quiz/</guid><description>--- primary_color: cyan secondary_color: lightgray text_color: black shuffle_questions: false shuffle_answers: true --- ## Basic collective comms Consider the following code snippet: ```java // obj.update() returns a bool, whether obj was updated or not. updated = Foam::returnReduce(obj.update(), andOp()); ``` The value of `updated` will: - [x] Be true if `obj` was updated on all processes. - [ ] Be true if `obj` was updated on at least one process. - [ ] Be always false, because `update()` is never called.</description></item><item><title>How do I send my own data?</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/03-send-custom-data/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/03-send-custom-data/</guid><description>Lecture video # üëã Content will be available on the the next training day Register at eveeno.com Module transcript Welcome again to this module where we finally move from simple types like booleans and integers to our own data structures and types.
We&amp;rsquo;ve actually touched on this topic in the general introduction to the point-to-point communication where we said OpenFOAM avoids the burden of MPI data types by choking everything through stream objects.</description></item><item><title>Application examples and advanced topics</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/04-advanced-topics/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/04-advanced-topics/</guid><description>Lecture video # üëã Content will be available on the the next training day Register at eveeno.com Module transcript Welcome to the last module of this workshop. We will conclude by studying some application examples which employ the concepts we have talked about, and expose some the most common issues you will encounter.
Our first application lies in the heart of any CFD software package: Solving PDEs over a decomposed mesh.</description></item><item><title>Sources and further reading</title><link>https://openfoam-parallelisation-course.github.io/workshop/chapters/05-sources/</link><pubDate>Tue, 25 Jan 2022 14:41:39 +0100</pubDate><guid>https://openfoam-parallelisation-course.github.io/workshop/chapters/05-sources/</guid><description>C. Augustine. Introduction to Parallel Programming with MPI and OpenMP. Source of the great ‚Äôpit stops‚Äô analogy. Oct. 2018. url: https://princetonuniversity.github.io/PUbootcamp/sessions/parallel-programming/Intro_PP_bootcamp_2018.pdf Fabio Baruffa. Improve MPI Performance by Hiding Latency. July 2020. url: https://www.intel.com/content/www/us/en/developer/articles/technical/overlap-computation-communication-hpc-applications.html Pavanakumar Mohanamuraly, Jan Christian Huckelheim, and Jens-Dominik Mueller. ‚ÄúHybrid Parallelisation of an Algorithmically Differentiated Adjoint Solver‚Äù. In: Proceedings of the VII European Congress on Computational Methods in Applied Sciences and Engineering (ECCOMAS Congress 2016). Institute of Structural Analysis and Antiseismic Research School of Civil Engineering National Technical University of Athens (NTUA) Greece, 2016.</description></item></channel></rss>